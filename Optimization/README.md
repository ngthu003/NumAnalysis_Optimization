This is a collection of my works on optimization, with a primary focus on *convex optimization*, including the topics of

1.  __Unconstrained__ vs. __Constrained__ optimization:
    1.  Objective: minimize f(x)
    2.  Equality conditions, for example h(x) = 0
    3.  Inequality conditions, for example g(x) <= 0

2.  __First-order__ methods:
    1.  Gradient descent
        1.  __[Standard Gradient descent](https://github.com/thn003/optimization_num_analysis/blob/master/Optimization/Gradient%20Descent%20Algorithm.ipynb)__
        2.  GD with Momentum
        3.  GD with Nesterov
    2.  Stochastic gradient descent
    3.  Conjugate descent
    4.  Dual ascent
    5.  Method of multipliers

3.  __Second-order__ methods:
    1.  Newton's method
