This is a collection of my works on optimization, with a primary focus on *convex optimization*, including the topics of

1.  __Unconstrained__ vs. __Constrained__ optimization
2.  __First-order__ methods:
    1.  Gradient descent
        1.  Standard Gradient descent
        2.  GD with Momentum
        3.  GD with Nesterov
    2.  Stochastic gradient descent
    3.  Conjugate descent
3.  __Second-order__ methods:
    1.  Newton's method
